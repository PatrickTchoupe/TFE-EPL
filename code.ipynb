{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.hub import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "## uncomment if you want to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" config.set_cache_directory(os.path.expanduser('~/openml/cache'))\\nconfig.cachedir = '~/openml/cache' \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openml import datasets, tasks, runs, flows, config, study\n",
    "from openml.datasets import edit_dataset, fork_dataset, get_dataset\n",
    "from openml.tasks import TaskType\n",
    "\n",
    "config.apikey = os.getenv('OPEN_ML_KEY')\n",
    "config.server = 'https://www.openml.org/api/v1'\n",
    "\"\"\" config.set_cache_directory(os.path.expanduser('~/openml/cache'))\n",
    "config.cachedir = '~/openml/cache' \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datasets.list_datasets(output_format=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset by name\n",
    "dataset = datasets.get_dataset(61)\n",
    "\n",
    "# Get the data itself as a dataframe (or otherwise)\n",
    "data, _, _, _ = dataset.get_data(dataset_format=\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_A = torch.tensor([[1, 2],\n",
    "                         [3, 4],\n",
    "                         [5, 6]], dtype=torch.float32)\n",
    "\n",
    "tensor_B = torch.tensor([[7, 10],\n",
    "                         [8, 11], \n",
    "                         [9, 12]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 2])\n",
      "\n",
      "Output:\n",
      "tensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n",
      "        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n",
      "        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Output shape: torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "# Since the linear layer starts with a random weights matrix, let's make it reproducible (more on this later)\n",
    "torch.manual_seed(42)\n",
    "# This uses matrix multiplication\n",
    "linear = torch.nn.Linear(in_features=2, # in_features = matches inner dimension of input \n",
    "                         out_features=6) # out_features = describes outer value \n",
    "x = tensor_A\n",
    "output = linear(x)\n",
    "print(f\"Input shape: {x.shape}\\n\")\n",
    "print(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create *known* parameters\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "# Create data\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
    "y = weight * X + bias\n",
    "\n",
    "X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40, 10, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create train/test split\n",
    "train_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing \n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Linear Regression model class\n",
    "class LinearRegressionModel(nn.Module): # <- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.weights = nn.Parameter(torch.randn(1, # <- start with random weights (this will get adjusted as the model learns)\n",
    "                                                dtype=torch.float), # <- PyTorch loves float32 by default\n",
    "                                   requires_grad=True) # <- can we update this value with gradient descent?)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.randn(1, # <- start with random bias (this will get adjusted as the model learns)\n",
    "                                            dtype=torch.float), # <- PyTorch loves float32 by default\n",
    "                                requires_grad=True) # <- can we update this value with gradient descent?))\n",
    "\n",
    "    # Forward defines the computation in the model\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # <- \"x\" is the input data (e.g. training/testing features)\n",
    "        return self.weights * x + self.bias # <- this is the linear regression formula (y = m*x + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1288], requires_grad=True)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set manual seed since nn.Parameter are randomly initialzied\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s))\n",
    "model_0 = LinearRegressionModel()\n",
    "\n",
    "# Check the nn.Parameter(s) within the nn.Module subclass we created\n",
    "list(model_0.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with model\n",
    "with torch.inference_mode(): \n",
    "    y_preds = model_0(X_test)\n",
    "\n",
    "# Note: in older PyTorch code you might also see torch.no_grad()\n",
    "# with torch.no_grad():\n",
    "#   y_preds = model_0(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing samples: 10\n",
      "Number of predictions made: 10\n",
      "Predicted values:\n",
      "tensor([[0.3982],\n",
      "        [0.4049],\n",
      "        [0.4116],\n",
      "        [0.4184],\n",
      "        [0.4251],\n",
      "        [0.4318],\n",
      "        [0.4386],\n",
      "        [0.4453],\n",
      "        [0.4520],\n",
      "        [0.4588]])\n"
     ]
    }
   ],
   "source": [
    "# Check the predictions\n",
    "print(f\"Number of testing samples: {len(X_test)}\") \n",
    "print(f\"Number of predictions made: {len(y_preds)}\")\n",
    "print(f\"Predicted values:\\n{y_preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the loss function\n",
    "loss_fn = nn.L1Loss() # MAE loss is same as L1Loss\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), # parameters of target model to optimize\n",
    "                            lr=0.01) # learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Array:\n",
      "[[[ 1  2]\n",
      "  [ 3  4]\n",
      "  [ 5  6]\n",
      "  [ 7  8]]\n",
      "\n",
      " [[ 9 10]\n",
      "  [11 12]\n",
      "  [13 14]\n",
      "  [15 16]]\n",
      "\n",
      " [[17 18]\n",
      "  [19 20]\n",
      "  [21 22]\n",
      "  [23 24]]]\n",
      "\n",
      "Transposed Array:\n",
      "[[[ 1  2]\n",
      "  [ 9 10]\n",
      "  [17 18]]\n",
      "\n",
      " [[ 3  4]\n",
      "  [11 12]\n",
      "  [19 20]]\n",
      "\n",
      " [[ 5  6]\n",
      "  [13 14]\n",
      "  [21 22]]\n",
      "\n",
      " [[ 7  8]\n",
      "  [15 16]\n",
      "  [23 24]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Créer un tableau tridimensionnel de forme (3, 4, 2)\n",
    "X = np.array([[[1, 2], [3, 4], [5, 6], [7, 8]],\n",
    "              [[9, 10], [11, 12], [13, 14], [15, 16]],\n",
    "              [[17, 18], [19, 20], [21, 22], [23, 24]]])\n",
    "\n",
    "# Afficher le tableau original\n",
    "print(\"Original Array:\")\n",
    "print(X)\n",
    "\n",
    "# Transposer le tableau en échangeant les axes 1 et 0\n",
    "X_transposed = X.transpose(1, 0, 2)\n",
    "\n",
    "# Afficher le tableau transposé\n",
    "print(\"\\nTransposed Array:\")\n",
    "print(X_transposed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['25', '50000', 'City', 'Electronics'], ['30', '60000', 'Suburb', 'Clothing'], ['35', '75000', 'City', 'Electronics'], ['22', '45000', 'Rural', 'Food'], ['28', '55000', 'Suburb', 'Electronics']]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Feature2VecPreEncoder(object):\n",
    "\n",
    "    def __init__(self, vector_size=100, window=5, min_count=1, workers=4):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        self.model = None\n",
    "        self.feature_columns = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Stocker les noms de colonnes des caractéristiques\n",
    "        self.feature_columns = X.columns.tolist()\n",
    "\n",
    "        # Entraîner le modèle Word2Vec sur les données tabulaires\n",
    "        sentences = [X[self.feature_columns].iloc[i].astype(str).tolist() for i in range(len(X))]\n",
    "        print(sentences)\n",
    "        \n",
    "        self.model = Word2Vec(sentences=sentences, vector_size=self.vector_size, window=self.window,\n",
    "                              min_count=self.min_count, workers=self.workers)\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Vérifier si les colonnes de caractéristiques correspondent à celles utilisées pendant l'entraînement\n",
    "        if set(self.feature_columns) != set(X.columns):\n",
    "            raise ValueError(\"Les colonnes de caractéristiques ne correspondent pas à celles utilisées pendant l'entraînement.\")\n",
    "\n",
    "        # Obtenir les embeddings des caractéristiques\n",
    "        feature_embeddings = np.array([self.model.wv[X.iloc[i].astype(str).tolist()] for i in range(len(X))])\n",
    "        return feature_embeddings\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# Créer un DataFrame factice\n",
    "data = {'age': [25, 30, 35, 22, 28],\n",
    "        'income': [50000, 60000, 75000, 45000, 55000],\n",
    "        'location': ['City', 'Suburb', 'City', 'Rural', 'Suburb'],\n",
    "        'product_type': ['Electronics', 'Clothing', 'Electronics', 'Food', 'Electronics']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Créer une instance de Feature2VecPreEncoder\n",
    "feature2vec_encoder = Feature2VecPreEncoder(vector_size=50, window=3, min_count=1, workers=4)\n",
    "\n",
    "# Entraîner le pré-encodeur\n",
    "feature2vec_encoder.fit(df)\n",
    "\n",
    "# Transformer les caractéristiques en embeddings\n",
    "df_embeddings = feature2vec_encoder.transform(df)\n",
    "\n",
    "# Afficher les embeddings résultants\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import OpenmlDataset, OpenmlDatasetLoader\n",
    "from datasets import dataset_train_test_split\n",
    "from preencoder import PreEncoder \n",
    "from models import ToyModel          \n",
    "### example task_ids ###\n",
    "\n",
    "# 361066 bank-marketing\n",
    "# 361076 wine_quality\n",
    "# 361085 sulfur\n",
    "# 361088 superconduct\n",
    "# 361089 california\n",
    "# 361110 electricity\n",
    "# 361111 eye_movements\n",
    "# 361112 KDDCup09_upselling\n",
    "# 361114 rl\n",
    "# 361116 compass\n",
    "# 361099 Bike_Sharing_Demand\n",
    "# 361102 house_sales\n",
    "\n",
    "task_id =  361099 #361076\n",
    "\n",
    "dataset_loader = OpenmlDatasetLoader()\n",
    "dataset = dataset_loader.load(task_id)\n",
    "dataset.print_info()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
